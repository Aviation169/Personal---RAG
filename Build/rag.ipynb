{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. Load and Process Documents\n",
    "doc_folder = \"personal_docs\"  # Your folder\n",
    "documents = []\n",
    "metadata = []\n",
    "\n",
    "def load_documents(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if filename.endswith((\".txt\", \".py\", \".pdf\")):\n",
    "            try:\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    reader = PdfReader(filepath)\n",
    "                    text = \"\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "                    if not text.strip():\n",
    "                        print(f\"Warning: No text extracted from {filename}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read()\n",
    "                documents.append(text)\n",
    "                metadata.append({\"source\": filename, \"type\": \"pdf\" if filename.endswith(\".pdf\") else \"text\"})\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {filename}: {str(e)}\")\n",
    "\n",
    "load_documents(os.path.expanduser(doc_folder))\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "doc_chunks = text_splitter.create_documents(documents, metadatas=metadata)\n",
    "documents = [doc.page_content for doc in doc_chunks]\n",
    "chunk_metadata = [doc.metadata for doc in doc_chunks]\n",
    "\n",
    "# 2. Set Up Retriever\n",
    "retriever = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "doc_embeddings = retriever.encode(documents, batch_size=32, show_progress_bar=True)\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "def retrieve(query, k=2):  # Reduced k to limit context\n",
    "    query_embedding = retriever.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [(documents[idx], chunk_metadata[idx]) for idx in indices[0]]\n",
    "\n",
    "# 3. Set Up Generator\n",
    "model_name = \"G:\\My Drive\\llama-3.2-3b-instruct\"  # Your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "def generate_response(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"[{meta['source']}]: {doc[:200]}...\" for doc, meta in retrieved_docs])  # Limit doc length\n",
    "    prompt = (\n",
    "        f\"You are my personal AI assistant, specialized in my research on Progressive Neural Networks (PNNs) and LLaMA. \"\n",
    "        f\"Using only the provided documents, explain my work concisely and technically, citing sources like '[filename]'. \"\n",
    "        f\"If the query is about a paper, summarize its main contribution and key methods. \"\n",
    "        f\"Documents:\\n{context}\\n\\nQuery: {query}\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=250, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Robust parsing\n",
    "    answer = response.split(\"Answer:\")[-1].strip() if \"Answer:\" in response else response.strip()\n",
    "    # Fallback if parsing fails\n",
    "    if not answer or answer.startswith(\"You are my personal AI assistant\"):\n",
    "        answer = \"Unable to generate a specific answer. Please check document content or query phrasing.\"\n",
    "    return answer\n",
    "\n",
    "# 4. RAG Pipeline\n",
    "def personal_rag(query, k=2):\n",
    "    retrieved_docs = retrieve(query, k)\n",
    "    response = generate_response(query, retrieved_docs)\n",
    "    return response, retrieved_docs\n",
    "\n",
    "# 5. Test the Pipeline\n",
    "query = \"Explain about the authors of this paper\"\n",
    "answer, sources = personal_rag(query)\n",
    "print(f\"Query: {query}\\nAnswer: {answer}\\nSources: {[meta['source'] for _, meta in sources]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "import gc # Import the garbage collector module\n",
    "\n",
    "\n",
    "def process_query(query):\n",
    "  \"\"\"Processes the query and returns the response.\"\"\"\n",
    "\n",
    "  # Load and split document\n",
    "  loader = TextLoader(\"Latest_news.txt\", encoding=\"utf-8\")\n",
    "  docs = loader.load()\n",
    "  splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "  chunks = splitter.split_documents(docs)\n",
    "\n",
    "  # Create vector database\n",
    "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "  vector_db = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "  # Similarity search\n",
    "  retrieved_docs = vector_db.similarity_search(query, k=3)\n",
    "  context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "  # Generate response using LLM\n",
    "  llm = pipeline(\"text-generation\", model=\"G:\\My Drive\\llama-3.2-3b-instruct\")\n",
    "  prompt = f\"Answer the question based on the context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "  response = llm(prompt, max_new_tokens=300)[0][\"generated_text\"]\n",
    "\n",
    "  # Clean up memory\n",
    "  del loader, docs, splitter, chunks, embedding_model, vector_db, retrieved_docs, context, llm, prompt\n",
    "  gc.collect()  # Explicitly trigger garbage collection\n",
    "\n",
    "  return response\n",
    "\n",
    "# Example usage\n",
    "query = \"Thanks for your information\"\n",
    "response = process_query(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
