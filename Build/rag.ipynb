{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9d8dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  8.54it/s]\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context:\n",
      "Hi, I’m M.Ajay sivakumar, and I’m from Tamil Nadu, India. I’m an AI enthusiast deeply passionate about artificial intelligence and its potential to shape the future. I’ve been building intelligent systems like transformers, self-learning agents, and research-driven LLMs from scratch.\n",
      "I’ve also worked on fine-tuning models, creating agents, and exploring the foundations of AGI. \n",
      "My ultimate aim is to work at OpenAI and contribute to developing responsible, powerful AI.\n",
      "My ultimate aim is to work at OpenAI and contribute to developing responsible, powerful AI. \n",
      "I believe in learning continuously, pushing my limits, and stepping out of my comfort zone — which is why I also dream of working in the U.S. or Europe to gain global experience. \n",
      "Every day, I’m growing not just as a developer, but as someone who wants to make a real impact in the world through AI.\n",
      "personal stuggels: \"The Chronicles of Lord Ajay: A Heart That Chose Love Even in Chaos\"\n",
      "But his heart never turned cold.\n",
      "\n",
      "Question: Thanks for your informations\n",
      "Answer: You're welcome! It seems there's a bit of a mismatch in the provided text. The initial message is about the person's passion for AI and their career aspirations, while the second part appears to be a personal story about someone named Lord Ajay. Could you please clarify which one you'd like to discuss further? I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "import gc\n",
    "\n",
    "# Global memory dictionary for storing session-wise history\n",
    "session_memory = {}\n",
    "\n",
    "def process_query(query, session_id=\"default\"):\n",
    "    \"\"\"Processes the query and returns the response, with session memory.\"\"\"\n",
    "\n",
    "    # If session_id not in memory, initialize it\n",
    "    if session_id not in session_memory:\n",
    "        session_memory[session_id] = {\n",
    "            \"history\": [],  # Stores (query, response) tuples\n",
    "        }\n",
    "\n",
    "    # Load and split document\n",
    "    loader = TextLoader(\"Latest_news.txt\", encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # Create vector database\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_db = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "    # Similarity search\n",
    "    retrieved_docs = vector_db.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Add previous Q&A from this session to the context\n",
    "    previous_context = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in session_memory[session_id][\"history\"]])\n",
    "    full_context = f\"{previous_context}\\n{context}\" if previous_context else context\n",
    "\n",
    "    # Generate response using LLM\n",
    "    llm = pipeline(\"text-generation\", model=\"G:\\My Drive\\llama-3.2-3b-instruct\")\n",
    "    prompt = f\"Answer the question based on the context:\\n{full_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = llm(prompt, max_new_tokens=300)[0][\"generated_text\"]\n",
    "\n",
    "    # Store in session history\n",
    "    session_memory[session_id][\"history\"].append((query, response))\n",
    "\n",
    "    # Clean up memory\n",
    "    del loader, docs, splitter, chunks, embedding_model, vector_db, retrieved_docs, context, llm, prompt\n",
    "    gc.collect()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"Thanks for your informations\"\n",
    "response = process_query(query, session_id=\"user123\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
