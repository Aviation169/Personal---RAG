ğŸ¤–Personal RAG prototypeğŸ¤–
-

**âš ï¸For production, it required a lot of enhancementâš ï¸**

1ï¸âƒ£A Streamlit-based web application implementing **Retrieval-Augmented Generation (RAG)** to answer questions about uploaded news content. 
The app retrieves relevant text from a user-provided .txt file using FAISS and LangChain, then generates answers with a language model.

ğŸ“ƒOverviewğŸ“ƒ
-

This app leverages the RAG framework:

ğŸ«´Retrieval: Uses FAISS vector store and Hugging Face embeddings `(all-MiniLM-L6-v2)` to find relevant text chunks from an uploaded file.

ğŸ“‚Augmentation: Combines retrieved context with the userâ€™s question to form a prompt.

ğŸ“Generation: Generates answers using a language model `(default: Llama3.2 for testing)`.

ğŸ–¨ï¸FeaturesğŸ–¨ï¸
-

â‚¬ Upload a `.txt file` to query its news content.

â‚¬ Ask questions and get answers powered by RAG.

â‚¬ Clean UI with a sidebar for file uploads, styled answer display, and a reset option.

â‚¬ Displays the source file name (e.g., "Answer based on: Latest_news.txt") instead of raw context.

â¬Prerequisitesâ¬
-

`Python 3.8+`

`Git installed`

`A GitHub account`

(Optional) A local or Hugging Face language model for generation

ğŸ§ªUsageğŸ§ª
-

1ï¸âƒ£Run the Streamlit app:

`streamlit run app.py`

2ï¸âƒ£Open your browser to `http://localhost:8501`.

3ï¸âƒ£In the sidebar, upload a UTF-8 encoded .txt file containing news content.

4ï¸âƒ£Enter a question (e.g., "Is there any girl name mentioned here?").

5ï¸âƒ£Click "Get Answer" to see the RAG-generated response, with the file name as the source.

6ï¸âƒ£Click "Reset" to clear the query and answer.

ğŸ’€RAG ConfigurationğŸ’€
-

â†’Embedding Model: `all-MiniLM-L6-v2` for efficient text embeddings.

â†’Vector Store: FAISS with `chunk_size=500` and `chunk_overlap=100` for retrieval.

â†’Language Model: Default is `Llama3.2`. To use a custom model, update `UI.py and Rag.ipynb`:

```
llm = pipeline("text-generation", model="your/model/path")
```

â†’Example for Hugging Face (requires authentication):

```
llm = pipeline("text-generation", model="meta-llama/Llama-3.2-3B-Instruct", token="your_hf_token")
```

â†’File Encoding: Uploaded files must be UTF-8 encoded to avoid errors.

(â—'â—¡'â—)License
-

MIT License
